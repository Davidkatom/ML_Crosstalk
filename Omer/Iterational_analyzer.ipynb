{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-31T16:02:02.844585Z",
     "start_time": "2024-01-31T16:02:02.828783800Z"
    }
   },
   "outputs": [],
   "source": [
    "from pkg_ML_V1 import *\n",
    "\n",
    "\n",
    "def generate_output_columns(numOfQubits):\n",
    "    # Create lists for W_ and decay_ with the appropriate number of elements\n",
    "    W_columns = ['W_{}'.format(i) for i in range(numOfQubits)]\n",
    "    decay_columns = ['decay_{}'.format(i) for i in range(numOfQubits)]\n",
    "\n",
    "    # Create a list for J_ with one less element than W_ and decay_\n",
    "    J_columns = ['J_{}'.format(i) for i in range(numOfQubits - 1)]\n",
    "\n",
    "    # Combine the lists to form Output_columns\n",
    "    Output_columns = W_columns + J_columns + decay_columns\n",
    "    return Output_columns\n",
    "\n",
    "\n",
    "def printHistory(history):\n",
    "    print('loss:',history[1]['loss'][len(history[1]['loss']) - 1])\n",
    "    print('val_loss:',history[1]['val_loss'][len(history[1]['val_loss']) - 1])\n",
    "    print('percentage_error_0:',history[1]['percentage_error_0'][len(history[1]['percentage_error_0']) - 1])\n",
    "    print('val_percentage_error_0:',history[1]['val_percentage_error_0'][len(history[1]['val_percentage_error_0']) - 1])\n",
    "\n",
    "\n",
    "\n",
    "def create_relu_model(neurons_, learning_rate_, concatenated_inputs_):\n",
    "    hidden_layer_1 = tf.keras.layers.Dense(neurons_, activation='relu')(concatenated_inputs_)\n",
    "    hidden_layer_2 = tf.keras.layers.Dense(neurons_, activation='relu')(hidden_layer_1)\n",
    "    hidden_layer_3 = tf.keras.layers.Dense(neurons_, activation='relu')(hidden_layer_2)\n",
    "    hidden_layer_4 = tf.keras.layers.Dense(neurons_, activation='relu')(hidden_layer_3)\n",
    "    output = tf.keras.layers.Dense(len(Output_columns))(hidden_layer_4)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_)\n",
    "    # Metrics for monitoring the learning process\n",
    "    # Initialize an empty list to hold the metrics for the output\n",
    "    metrics = ['mean_squared_error', 'mean_absolute_percentage_error']\n",
    "    for i in range(len(Output_columns)):\n",
    "        metrics.append(percentage_error(i))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=metrics)\n",
    "\n",
    "    # print(\"Learning rate: \" + str(learning_rate_))\n",
    "    # print(\"Amount of neurons: \" + str(neurons_))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1630c82a2e6b9d58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rate = 10 ** -4\n",
    "epochs = 3\n",
    "batch_size = 2 ** 8\n",
    "neurons = 2 ** 9\n",
    "\n",
    "Output_data = {}\n",
    "\n",
    "\n",
    "# Specify the directory where the CSV files are located\n",
    "folder_path = 'Generated_Data'\n",
    "# List all files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Attempt to match the filename pattern\n",
    "        match = re.match(r\"Qubits_(\\d+)_Lines_(\\d+)_TS_(\\d+)_Shots_(\\d+)_MeanDecay_(\\d+)\", filename)\n",
    "        if match:\n",
    "            # Extract parameters from the filename\n",
    "            qubits, lines, TS, shots, meanDecay = match.groups()\n",
    "\n",
    "            # Convert strings to integers\n",
    "            qubits = int(qubits)\n",
    "            lines = int(lines)\n",
    "            TS = int(TS)\n",
    "            shots = int(shots)\n",
    "            meanDecay = int(meanDecay)\n",
    "\n",
    "            # Load the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            train_df = pd.read_csv(file_path)\n",
    "            Output_columns = generate_output_columns(qubits) # Generate the columns to drop\n",
    "            feature_columns = train_df.columns.drop(Output_columns)\n",
    "            inputs = {column: tf.keras.layers.Input(shape=(1,), name=column) for column in feature_columns}\n",
    "            concatenated_inputs = tf.keras.layers.concatenate(inputs.values())\n",
    "            # Prepare training data\n",
    "            train_features = {column: train_df[column] for column in inputs}\n",
    "            train_labels = train_df[Output_columns]\n",
    "            # Create Model:\n",
    "            print(\"Working on file:\",filename)\n",
    "            model = create_relu_model(neurons, learning_rate, concatenated_inputs)\n",
    "\n",
    "            history = train_model(model, train_features, train_labels,\n",
    "                      epochs, batch_size, validation_split=0.1)\n",
    "            printHistory(history)\n",
    "            Output_data[filename] = history\n",
    "print(Output_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c5a234c2bf58e183"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "418c6fbcbb966884"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
